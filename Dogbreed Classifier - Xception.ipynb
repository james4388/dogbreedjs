{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import xception\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.xception import preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from classes import CLASSES, LABELS_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (299, 299)\n",
    "base_model = xception.Xception(include_top=False, input_shape=(*image_size, 3), weights='imagenet', pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_data(label_file='./dataset/labels.csv',\n",
    "                    base_dir='./dataset/train/',\n",
    "                    train_dir=None,\n",
    "                    validation_dir=None, validation_split=0.1, ext='.jpg'):\n",
    "    # Read label file to get list of file name and label\n",
    "    data_by_class = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            filename, label = line.split(',')\n",
    "            filename = filename.strip()\n",
    "            label = label.strip()\n",
    "            if label not in data_by_class:\n",
    "                data_by_class[label] = []\n",
    "            data_by_class[label].append(filename)\n",
    "    # Create dirs\n",
    "    validation_split = 0.1\n",
    "    train_dir = train_dir or os.path.join(base_dir, 'train')\n",
    "    validation_dir = validation_dir or os.path.join(base_dir, 'validation')\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(validation_dir)\n",
    "    # Moving everything into its folder\n",
    "    for label, files in data_by_class.items():\n",
    "        label_dir_train = os.path.join(train_dir, label)\n",
    "        os.mkdir(label_dir_train)\n",
    "        label_dir_val = os.path.join(validation_dir, label)\n",
    "        os.mkdir(label_dir_val)\n",
    "\n",
    "        val_index = len(files) * validation_split\n",
    "        for i, filename in enumerate(files):\n",
    "            if ext:\n",
    "                filename += ext\n",
    "            old_path = os.path.join(base_dir, filename)\n",
    "            if os.path.isfile(old_path):\n",
    "                if (i + 1) < val_index:\n",
    "                    os.rename(old_path, os.path.join(label_dir_val, filename))\n",
    "                else:\n",
    "                    os.rename(old_path, os.path.join(label_dir_train, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze bottom layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "def get_top_model(base_model):\n",
    "    top_input = Input(shape=base_model.output_shape[1:])\n",
    "    x = Dense(1024, activation=\"relu\", name='dense1024')(top_input)\n",
    "    x = Dropout(0.5, name='drop05')(x)\n",
    "    x = Dense(512, activation=\"relu\", name='dense512')(x)\n",
    "    x = Dropout(0.2, name='drop02')(x)\n",
    "    output = Dense(len(CLASSES), activation=\"softmax\", name='final')(x)\n",
    "    model = Model(input=top_input, output=output)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer='adam',\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_final_model(base_model, top_model):\n",
    "    final_output = top_model(base_model.output)\n",
    "    final_model = Model(input=base_model.input, output=final_output)\n",
    "    final_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=SGD(lr=5e-5, momentum=0.9),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, path):\n",
    "    img = load_img(path, target_size=(299, 299))\n",
    "    img_arr = img_to_array(img)\n",
    "    inp = preprocess_input(img_arr)\n",
    "    inp = np.expand_dims(inp, axis=0)\n",
    "    preds = model.predict(inp)\n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9357 images belonging to 122 classes.\n",
      "Found 1015 images belonging to 122 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen =  ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './dataset/images/train/', \n",
    "    target_size=image_size,\n",
    "    batch_size=25,\n",
    "    class_mode='categorical', shuffle=False)\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    './dataset/images/validation//', \n",
    "    target_size=image_size,\n",
    "    batch_size=25,\n",
    "    class_mode='categorical', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = base_model.predict_generator(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('bottleneck_features-xcetp.h5', 'w')\n",
    "hf.create_dataset('bottleneck_features_train', data=bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_val = base_model.predict_generator(val_generator)\n",
    "hf.create_dataset('bottleneck_features_val', data=bottleneck_features_val)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('bottleneck_features-xcetp.h5', 'r')\n",
    "bottleneck_features_train = hf.get('bottleneck_features_train').value\n",
    "bottleneck_features_val = hf.get('bottleneck_features_val').value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train == bottleneck_features_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(train_generator.classes)\n",
    "Y_train = np.zeros((len(train_generator.classes), len(train_generator.class_indices)))\n",
    "Y_train[np.arange(len(train_generator.classes)), indices] = 1\n",
    "indices = np.array(val_generator.classes)\n",
    "Y_val = np.zeros((len(val_generator.classes), len(train_generator.class_indices)))\n",
    "Y_val[np.arange(len(val_generator.classes)), indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhutrinh/dogbreeed/env/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fi...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "top_model = get_top_model(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "9357/9357 [==============================] - 5s 558us/step - loss: 1.3407 - acc: 0.6846\n",
      "Epoch 2/40\n",
      "9357/9357 [==============================] - 4s 469us/step - loss: 0.5153 - acc: 0.8483\n",
      "Epoch 3/40\n",
      "9357/9357 [==============================] - 4s 480us/step - loss: 0.4377 - acc: 0.8689\n",
      "Epoch 4/40\n",
      "9357/9357 [==============================] - 5s 492us/step - loss: 0.3638 - acc: 0.8838\n",
      "Epoch 5/40\n",
      "9357/9357 [==============================] - 4s 475us/step - loss: 0.3387 - acc: 0.8923\n",
      "Epoch 6/40\n",
      "9357/9357 [==============================] - 5s 496us/step - loss: 0.2897 - acc: 0.9084\n",
      "Epoch 7/40\n",
      "9357/9357 [==============================] - 5s 507us/step - loss: 0.2603 - acc: 0.9113\n",
      "Epoch 8/40\n",
      "9357/9357 [==============================] - 5s 518us/step - loss: 0.2477 - acc: 0.9200\n",
      "Epoch 9/40\n",
      "9357/9357 [==============================] - 5s 528us/step - loss: 0.2332 - acc: 0.9232\n",
      "Epoch 10/40\n",
      "9357/9357 [==============================] - 5s 555us/step - loss: 0.2220 - acc: 0.9258\n",
      "Epoch 11/40\n",
      "9357/9357 [==============================] - 5s 543us/step - loss: 0.1974 - acc: 0.9301\n",
      "Epoch 12/40\n",
      "9357/9357 [==============================] - 5s 536us/step - loss: 0.2097 - acc: 0.9311\n",
      "Epoch 13/40\n",
      "9357/9357 [==============================] - 5s 535us/step - loss: 0.1889 - acc: 0.9365\n",
      "Epoch 14/40\n",
      "9357/9357 [==============================] - 5s 551us/step - loss: 0.1686 - acc: 0.9442\n",
      "Epoch 15/40\n",
      "9357/9357 [==============================] - 5s 563us/step - loss: 0.1692 - acc: 0.9451\n",
      "Epoch 16/40\n",
      "9357/9357 [==============================] - 6s 598us/step - loss: 0.1707 - acc: 0.9444\n",
      "Epoch 17/40\n",
      "9357/9357 [==============================] - 6s 605us/step - loss: 0.1457 - acc: 0.9522\n",
      "Epoch 18/40\n",
      "9357/9357 [==============================] - 5s 575us/step - loss: 0.1459 - acc: 0.9491\n",
      "Epoch 19/40\n",
      "9357/9357 [==============================] - 5s 566us/step - loss: 0.1500 - acc: 0.9486\n",
      "Epoch 20/40\n",
      "9357/9357 [==============================] - 5s 551us/step - loss: 0.1398 - acc: 0.9548\n",
      "Epoch 21/40\n",
      "9357/9357 [==============================] - 5s 580us/step - loss: 0.1478 - acc: 0.9528\n",
      "Epoch 22/40\n",
      "9357/9357 [==============================] - 6s 591us/step - loss: 0.1381 - acc: 0.9543\n",
      "Epoch 23/40\n",
      "9357/9357 [==============================] - 5s 553us/step - loss: 0.1331 - acc: 0.9598\n",
      "Epoch 24/40\n",
      "9357/9357 [==============================] - 5s 543us/step - loss: 0.1225 - acc: 0.9590\n",
      "Epoch 25/40\n",
      "9357/9357 [==============================] - 5s 570us/step - loss: 0.1337 - acc: 0.9565\n",
      "Epoch 26/40\n",
      "9357/9357 [==============================] - 5s 551us/step - loss: 0.1134 - acc: 0.9624\n",
      "Epoch 27/40\n",
      "9357/9357 [==============================] - 5s 558us/step - loss: 0.1093 - acc: 0.9634\n",
      "Epoch 28/40\n",
      "9357/9357 [==============================] - 5s 567us/step - loss: 0.1234 - acc: 0.9607\n",
      "Epoch 29/40\n",
      "9357/9357 [==============================] - 5s 576us/step - loss: 0.1284 - acc: 0.9577\n",
      "Epoch 30/40\n",
      "9357/9357 [==============================] - 6s 589us/step - loss: 0.1044 - acc: 0.9656\n",
      "Epoch 31/40\n",
      "9357/9357 [==============================] - 6s 621us/step - loss: 0.0990 - acc: 0.9677\n",
      "Epoch 32/40\n",
      "9357/9357 [==============================] - 6s 655us/step - loss: 0.1100 - acc: 0.9642\n",
      "Epoch 33/40\n",
      "9357/9357 [==============================] - 7s 717us/step - loss: 0.1164 - acc: 0.9641\n",
      "Epoch 34/40\n",
      "9357/9357 [==============================] - 7s 752us/step - loss: 0.1183 - acc: 0.9638\n",
      "Epoch 35/40\n",
      "9357/9357 [==============================] - 7s 774us/step - loss: 0.0940 - acc: 0.9696\n",
      "Epoch 36/40\n",
      "9357/9357 [==============================] - 7s 774us/step - loss: 0.1097 - acc: 0.9646\n",
      "Epoch 37/40\n",
      "9357/9357 [==============================] - 8s 851us/step - loss: 0.1107 - acc: 0.9667\n",
      "Epoch 38/40\n",
      "9357/9357 [==============================] - 8s 874us/step - loss: 0.0965 - acc: 0.9689\n",
      "Epoch 39/40\n",
      "9357/9357 [==============================] - 8s 905us/step - loss: 0.1052 - acc: 0.9688\n",
      "Epoch 40/40\n",
      "9357/9357 [==============================] - 10s 1ms/step - loss: 0.0957 - acc: 0.9718\n"
     ]
    }
   ],
   "source": [
    "history = top_model.fit(bottleneck_features_train, Y_train, batch_size=50, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1015/1015 [==============================] - 1s 569us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5873121570043376, 0.8817733994845687]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_model.evaluate(bottleneck_features_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.save_weights('top-xcep.w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhutrinh/dogbreeed/env/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"mo...)`\n"
     ]
    }
   ],
   "source": [
    "final_model = get_final_model(base_model, top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_by_index = {v:k for k,v in train_generator.class_indices.items()}\n",
    "CLASSES = [label_by_index[i] for i in label_by_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save('dogbreed-xcept.h5')\n",
    "# top_model = load_model('./dogbreed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('dogbreed-xcept.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.fit_generator(generator=train_generator, epochs=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(final_model, './js_model_xcept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import CustomObjectScope\n",
    "with CustomObjectScope({'relu6': mobilenet.relu6,'DepthwiseConv2D': mobilenet.DepthwiseConv2D}):\n",
    "    model = load_model('dogbreed-mobilenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(model, path):\n",
    "    p = predict(model, path)\n",
    "    top_indices = p.argsort()[-5:][::-1]\n",
    "    result = [(CLASSES[i], p[i],) for i in top_indices]\n",
    "    result.sort(key=lambda x: x[1], reverse=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sussex_spaniel', 1.0),\n",
       " ('clumber', 4.001788e-12),\n",
       " ('bloodhound', 1.1285548e-14),\n",
       " ('cocker_spaniel', 4.84357e-15),\n",
       " ('french_bulldog', 2.0436948e-16)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label(model, './dataset/images/validation/sussex_spaniel/181e91cb6caf6739478d06231faa053d.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to unfreeze layer before save to js otherwise not work\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('test-xcetp.wh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('test-xcetp.wh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import CustomObjectScope\n",
    "import tensorflowjs as tfjs\n",
    "with CustomObjectScope():\n",
    "    tfjs.converters.save_keras_model(model, './js_model_xcept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as K\n",
    "import tensorflow as tf\n",
    "sess = K.backend.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CLASSES[i] for i in np.argsort(predict(model, './cuteness1.jpg'))[::-1][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=base_model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
